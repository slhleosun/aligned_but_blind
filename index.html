<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race">
  <meta name="keywords" content="Alignment, Implicit Bias, LLM, Race Blindness">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤔</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2504.14379">
            The Geometry of Self-Verification in a Task-Specific Reasoning Model
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dnerf">Aligned but Blind</span>: <br>
          Alignment Increases Implicit Bias by Reducing Awareness of Race</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/uchicago.edu/lihao-sun">Lihao Sun</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.columbia.edu/~mcz/">Chengzhi Mao</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://valentinhofmann.github.io/">Valentin Hofmann</a><sup>34</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.xuechunzibai.com/">Xuechunzi Bai</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Chicago,</span>
            <span class="author-block"><sup>2</sup>Rutgers University,</span>
            <span class="author-block"><sup>3</sup>Allen Institute for AI,</span>
            <span class="author-block"><sup>4</sup>University of Washington</span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 10px;">
            <span class="author-block">ACL 2025 (Main) </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.00253"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.00253"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/slhleosun/aligned-but-blind"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="teaser-and-method">
  <div class="container is-max-desktop">
    <div class="teaser">
      <h3 class="title is-3">
        <b>Language model alignment unintentionally amplifies implicit racial biases by reducing their sensitivity to race concepts - akin to race blindness in humans.</b> 
      </h3>
      <div class="qualitative-understanding-images" style="margin-top: 1rem;"></div>
      <figure>
            <img src="static/images/selfie.jpg" alt="Alignment reduces explicit but increases implicit bias">
            <figcaption>Interpretations of LM associations with <span class="dnerf">black/white</span>.  Alignment makes the model treat them more like pure colors rather than racial groups - ignoring race-related associations can exacerbate bias. </figcaption>
          </figure>
    </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content ">
          <p>
            Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, <b>overlook racial concepts in early internal representations when the context is ambiguous</b>. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that <b>steering the model to be more aware of racial concepts effectively mitigates bias.</b> Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.
          </p>
        </div>
      </div>
    </div>

    <!-- =======================
     Behavioral Evidence
======================= -->
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Behavioral Evidence</h2>
      <div class="content">
        <p>
          We investigate racial bias in LMs—specifically, the tendency to associate 
          <span class="dnerf">black</span> with negative words and 
          <span class="dnerf">white</span> with positive ones. Bias is tested in two ways:
        </p>
        <ol style="text-align: left; display: inline-block; margin: 0 auto;">
          <li>
            <b>Explicit:</b> Directly asking the model to agree/disagree with a given association.
          </li>
          <li>
            <b>Implicit:</b> Prompting the model to associate 
            <span class="dnerf">black/white</span> with positive or negative words.
          </li>
        </ol>

        <div class="qualitative-understanding-images" style="margin-top: 1rem;">
          <figure>
            <img src="static/images/behavioral.jpg"
                 alt="Alignment consistently increases black implicit bias while reducing explicit bias.">
            <figcaption>
              Alignment reduces explicit bias but amplifies implicit racial bias.
            </figcaption>
          </figure>
        </div>

        <div style="border: 1px solid #ccc; padding: 10px; border-radius: 6px; background-color: #f9f9f9; margin-top: 1rem;">
          <p>
            &#x1F4A1; <b>Key Insight:</b> While alignment reduces explicit bias to near zero (8.1%), it increases implicit bias to 91.4%. 
          </p>
        </div>
      </div> <!-- /.content -->
    </div>   <!-- /.column -->
  </div>     <!-- /.columns -->
</div>       <!-- /.container -->



<!-- =======================
     Mechanistic Insights
======================= -->
<div class="container is-max-desktop" style="margin-top: 2rem;">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Mechanistic Insights</h2>

      <!-- Activation Patching -->
      <h3 class="title is-4">Interpreting Race versus Color: <br>Activation Patching</h3>
      <div class="content">
        <p>
          To explain this behavior, we use activation patching to test whether LMs represent 
          <span class="dnerf">black/white</span> as race or color in ambiguous settings.
        </p>
        <div class="qualitative-understanding-images" style="margin-top: 1rem;">
          <figure>
            <img src="static/images/activation_patching.jpg" alt="Activation patching results">
            <figcaption>
              Activation patching reveals aligned models treat <span class="dnerf">black/white</span> more as pure colors, not racial groups.
            </figcaption>
          </figure>
        </div>
        <ul style="text-align: left; display: inline-block; margin: 0 auto;"> 
          <li><b>Aligned models are 52.2% less likely to represent race internally.</b></li>
        </ul>
      </div> <!-- /.content -->

      <!-- SelfIE Analysis -->
      <h3 class="title is-4" style="margin-top: 2rem;">Visualizing Latent Bias: <br>SelfIE Analysis</h3>
      <div class="content">
        <p>
          To explore whether stronger associations beyond the race/color binary might be present, 
          we applied <a href="https://selfie.cs.columbia.edu/"><span class="dnerf">SelfIE</span></a> - an open-ended natural language 
          interpretation method that translates internal embeddings into text.
        </p>
        <div class="qualitative-understanding-images" style="margin-top: 1rem;">
          <figure>
            <img src="static/images/selfie_detail.jpg" alt="SelfIE Interpretations">
            <figcaption>
              Examples of SelfIE interpretations for <span class="dnerf">black/white</span>
            </figcaption>
          </figure>
        </div>
        <ul style="text-align: left; display: inline-block; margin: 0 auto; margin-top: 1rem;">
          <li>
            SelfIE interpretations of <span class="dnerf">black/white</span> fell into three categories: color, race, or nonsensical outputs 
            (e.g., repeating the instruction).
          </li>
          <li>
            Consistent with activation patching results, 
          <b>the aligned model produced 74.4% fewer race-related interpretations than the base model on implicit prompts.</b>
          </li>
        </ul>
        <div style="border: 1px solid #ccc; padding: 10px; border-radius: 6px; background-color: #f9f9f9; margin-top: 1rem;">
          <p>
            &#x1F4A1; <b>Key Insight:</b> Aligned LMs failed to robustly represent race concepts in the face of ambiguity, exhibiting race blindness. Not representing race likely fails to activate safety guardrails, leading to unintended biases.
          </p>
        </div>
      </div> <!-- /.content -->
    </div>   <!-- /.column -->
  </div>     <!-- /.columns -->
</div>       <!-- /.container -->



<!-- =======================
     Causal Intervention: Strengthening Race Associations
======================= -->
<div class="container is-max-desktop" style="margin-top: 2rem;">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Causal Intervention: <br>Strengthening Race Associations</h2>

      <div class="content">
        <p>
          Unlike traditional debiasing that removes racial concepts, we 
          <b>add racial awareness</b> to reduce bias - similar to how acknowledging race reduces human colorblind racism. 
          The effectiveness of this approach also provides causal support for our interpretability findings.
        </p>

        <!-- Embedding Intervention via Steering -->
        <h3 class="title is-4" style="margin-top: 2rem;">Embedding Intervention via Steering</h3>
        <div class="content">
          <ul style="text-align: left; display: inline-block; margin: 0 auto; margin-top: 1rem;">
            <li>
              <b>Injecting race‐aware activations</b> (from disambiguated prompts like 
              <span class="dnerf">“Race: black and white”</span>) 
              <b>reduces implicit bias by 26–42%</b>, especially when applied to early layers.
            </li>
          </ul>
        </div> <!-- /.content -->
        <div class="qualitative-understanding-images" style="margin-top: 1rem;">
          <figure>
            <img src="static/images/intervention.jpg" alt="Causal results">
            <figcaption>
              Strengthening race associations reduces implicit and explicit bias in LMs.
            </figcaption>
          </figure>
        </div>
        <!-- Weight Intervention via LoRA Fine‐tuning -->
        <h3 class="title is-4" style="margin-top: 2rem;">Weight Intervention via LoRA Fine-tuning</h3>
        <div class="content">
          <ul style="text-align: left; display: inline-block; margin: 0 auto; margin-top: 1rem;">
            <li>
              We fine-tune models using LoRA on prompts where <span class="dnerf">black/white</span> are used ambiguously but labeled with racial meaning.
            </li>
            <li>
              <b>Targeted early-layer fine-tuning cuts implicit bias from 97.3% to 42.4% and also reduces explicit bias.</b>
            </li>
          </ul>

          <div style="border: 1px solid #ccc; padding: 10px; border-radius: 6px; background-color: #f9f9f9; margin-top: 1rem;">
            <p>
              &#x1F4A1; <b>Key Insight:</b> Enhancing the model's awareness of racial concepts effectively reduces implicit bias.
            </p>
          </div>
        </div> <!-- /.content (Weight Intervention) -->
      </div>   <!-- /.content (Causal Intervention intro) -->
    </div>     <!-- /.column -->
  </div>       <!-- /.columns -->
</div>         <!-- /.container -->



<!-- =======================
     Implications
======================= -->
<div class="container is-max-desktop" style="margin-top: 2rem;">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Implications</h2>
      <div class="content">
        <p>
          Our findings suggest a broader class of alignment failure: <b>debiasing strategies 
          suppress sensitive concepts, they can unintentionally reduce a model's ability to detect bias, 
          which inadvertently exacerbates bias. Rather than removing sensitive concepts, reinforcing awareness of these concepts 
          in language models can mitigate biases. </b>Future research could extend similar methodologies to other bias domains 
          and investigate the pretraining origins of harmful associations, enhancing deeper alignment strategies across broader contexts.
        </p>
      </div> <!-- /.content -->
    </div>   <!-- /.column -->
  </div>     <!-- /.columns -->
</div>       <!-- /.container -->


        
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content ">
          <p>
            <b>Race Blindness and Implicit Bias</b><br>
            Human psychology research on colorblind racism
            <ul>
              <li><a href="https://www.annualreviews.org/doi/10.1146/annurev-psych-073115-103235">The Psychology of Racial Colorblindness</a></li>
              <li><a href="https://psycnet.apa.org/record/2012-13209-001">Racial Color Blindness: Emergence, Practice, and Implications</a></li>
            </ul>
            Implicit bias in language models
            <ul>
              <li><a href="https://arxiv.org/abs/2402.04105">Measuring Implicit Bias in Explicitly Unbiased Large Language Models</a></li>
              <li><a href="https://www.nature.com/articles/s41586-024-07856-5">AI generates covertly racist decisions about people based on their dialect</a></li>
            </ul>
          </p>
          <p>
            <b>Mechanistic interpretability and intervention</b><br>
            Mechanistic interpretability for bias
            <ul>
              <li><a href="https://arxiv.org/abs/2403.10949">SelfIE: Self-Interpretation of Large Language Model Embeddings</a></li>
              <li><a href="https://arxiv.org/abs/2401.06102">Patchscopes: A Unifying Framework for Inspecting Hidden Representations</a></li>
            </ul>
            Intervention methods
            <ul>
              <li><a href="https://arxiv.org/abs/2308.10248">Activation Engineering</a></li>
              <li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li>
              <li><a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a></li>
            </ul>
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
    
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{sun2025aligned,
  title={Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race},
  author={Sun, Lihao and Mao, Chengzhi and Hofmann, Valentin and Bai, Xuechunzi},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Webpage template adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and 
            <a href="https://selfie.cs.columbia.edu/">SelfIE</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>